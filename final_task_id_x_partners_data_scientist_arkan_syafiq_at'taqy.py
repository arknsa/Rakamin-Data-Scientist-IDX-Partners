# -*- coding: utf-8 -*-
"""Final  Task_ID/X Partners_Data Scientist_Arkan Syafiq At'taqy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-BTxenre_WhJkrMK3VcBkU6dAQUuZTp2

# ID/X Partners Data Scientist Project-Based Internship Program Final Task <br> by [Arkan Syafiq At'taqy](https://www.linkedin.com/in/arkan-attaqy/)

## Introduction

As a Data Scientist at ID/X Partners, I'm involved in a project from a lending company (multifinance), where my client wants to increase accuracy in assessing and managing credit risk, so that it can optimize their business decisions and reduce potential loss.

## Goals

The goal is to develop machine learning models which can predict credit risk based on dataset provided, which includes loan data approved and rejected. In developing the model too need to carry out several stages starting with [Data Understanding](https://colab.research.google.com/drive/1-BTxenre_WhJkrMK3VcBkU6dAQUuZTp2#scrollTo=FVxJ3CpzV65Y&line=1&uniqifier=1),  [Exploratory Data Analysis (EDA)](https://colab.research.google.com/drive/1-BTxenre_WhJkrMK3VcBkU6dAQUuZTp2#scrollTo=8i4_ZRE6ZU8g), [Data Preparation](https://colab.research.google.com/drive/1-BTxenre_WhJkrMK3VcBkU6dAQUuZTp2#scrollTo=LSPvhtuiZ8lU), [Data Modeling](https://colab.research.google.com/drive/1-BTxenre_WhJkrMK3VcBkU6dAQUuZTp2#scrollTo=96kou99hqQEt) and [Evaluation](https://colab.research.google.com/drive/1-BTxenre_WhJkrMK3VcBkU6dAQUuZTp2#scrollTo=3jWyb268AIqW).

# Data Understanding üîé

* Analyze the given dataset and make a summary
about the structure of the dataset, including the number of rows and columns.
* Identify and understand each attribute or data column.
* Conduct initial exploration of variable distributions, descriptive statistics, and general patterns in the data.

## Import Library
"""

pip install shap

pip install optuna

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore")
from sklearn.preprocessing import OneHotEncoder
import plotly.express as px
import shap

from sklearn.preprocessing import MinMaxScaler, StandardScaler
from scipy.stats import skew
from scipy.stats import chi2_contingency
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve

# Machine learning algorithms
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier
from lightgbm import LGBMClassifier
import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,BatchNormalization,Dropout
import os
from sklearn.base import ClassifierMixin
from sklearn.model_selection import StratifiedKFold, cross_val_predict

#for hypertuning
import optuna
from collections import Counter
from sklearn.model_selection import RandomizedSearchCV,GridSearchCV,RepeatedStratifiedKFold

# for model evaluation
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, classification_report, roc_auc_score, cohen_kappa_score, balanced_accuracy_score, roc_curve


#Mount Drive
from google.colab import drive
drive.mount('/content/drive/')

"""## Read Data"""

df = pd.read_csv("/content/drive/MyDrive/Rakamin Academy/Data Scientist - ID X Partners/dataset/loan_data_2007_2014.csv")
df.head()

"""## Data Dimension"""

print("Dataset shape: ",df.shape)

"""**What we found?üí°**

* Dataset:
<br> There are 466,285 rows of data (entries) in the dataset and there are 75 columns (features) that store information or attributes about each entry in the dataset.

## Information of Dataset
"""

df.info()

"""There are 17 Null Columns

## Column
"""

# Identify the data types of columns
column_data_types = df.dtypes

# Count the numerical and categorical columns
numerical_count = 0
categorical_count = 0

for column_name, data_type in column_data_types.items():
    if np.issubdtype(data_type, np.number):
        numerical_count += 1
    else:
        categorical_count += 1

# Print the counts
print(f"There are {numerical_count} Numerical Columns in Train dataset")
print(f"There are {categorical_count} Categorical Columns in Train dataset")

"""## Descriptive Statistic"""

# Desc stat of dataset numeric cols
df.describe().T

# Desc stat of train dataset cat cols
df.describe(include="O").T

# Check unique values of cat cols
cat_cols = df.select_dtypes(include="O").columns

for column in cat_cols:
    print('Unique values of ', column, set(df[column]))

"""## Check Duplicate"""

print("Duplicates in Train Dataset: ",df.duplicated().sum())

"""## Check Missing Value"""

print("Checking Null Values in Train Dataset")
missing_data = df.isnull().sum().to_frame().rename(columns={0:"Total No. of Missing Values"})
missing_data["% of Missing Values"] = round((missing_data["Total No. of Missing Values"]/len(df))*100,2)

# Menyortir DataFrame berdasarkan kolom '% of Missing Values' dalam urutan menurun
missing_data = missing_data.sort_values(by="% of Missing Values", ascending=False)

missing_data

"""**What we found?üí°**

There are missing values in the data.<br><br>
**Nilai yang hilang > 70%:**
* annual_inc_joint               
* dti_joint                    
* verification_status_joint      
* open_acc_6m                   
* open_il_6m                     
* open_il_12m                    
* open_il_24m                  
* mths_since_rcnt_il
* total_bal_il           
* il_util                  
* open_rv_12m                    
* open_rv_24m                    
* max_bal_bc                     
* all_util                       
* inq_fi                         
* total_cu_tl                    
* inq_last_12m
* mths_since_last_record
* mths_since_last_major_derog
* desc <br><br>

**Nilai yang hilang < 70%:**
* mths_since_last_delinq
* next_payment_d
* tot_cur_bal
* tot_coll_amt
* total_rev_hi_lim
* emp_title
* emp_length
* and 13 other variables that have missing values between 0.001% - 0.08%

## Check Outlier
"""

print("Checking Outliers in Train Dataset")

# Fungsi untuk mendeteksi outlier berdasarkan IQR
def detect_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    return outliers

# Daftar kolom numerik dalam dataframe
numeric_cols = df.select_dtypes(include='number').columns.tolist()

outliers_data = []

for col in numeric_cols:
    outliers = detect_outliers(df, col)
    outliers_data.append({"Column": col, "No. of Outliers": len(outliers)})

outliers_data = pd.DataFrame(outliers_data)
outliers_data = outliers_data.sort_values(by="No. of Outliers", ascending=False)
outliers_data

"""**What we found?üí°**

There are outliers in the data

# Exploratory Data Analysis üìä

* Visualize data using graphs and plots to understand the relationship between variables.
* Analyze the correlation between features in the dataset.
* Conduct univariate and bivariate analysis for gain deeper insight into the data.

## Correlation Between Features
"""

# Deteksi tipe data kolom
kategorikal_cols = df.select_dtypes(include=['object']).columns
numerikal_cols = df.select_dtypes(exclude=['object']).columns

print("Kolom Kategorikal:")
print(kategorikal_cols)

print("\nKolom Numerik:")
print(numerikal_cols)

jumlah_kategorikal_cols = len(kategorikal_cols)
jumlah_numerikal_cols = len(numerikal_cols)

print("\nJumlah Kolom Kategorikal:", jumlah_kategorikal_cols)
print("Jumlah Kolom Numerik:", jumlah_numerikal_cols)

categorical_cols = ['term', 'grade', 'home_ownership', 'emp_length', 'pymnt_plan', 'purpose', 'title', 'verification_status']
numerical_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'delinq_2yrs', 'open_acc', 'total_acc', 'revol_util',
                'mths_since_last_delinq', 'collections_12_mths_ex_med']

subset_df = df[numerical_cols]

# Analisis Korelasi
correlation_matrix = subset_df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

# Mengevaluasi hubungan statistik antara variabel kategorikal dan variabel target
for col in categorical_cols:
    cross_tab = pd.crosstab(df[col], df['loan_status'])
    chi2, p, _, _ = chi2_contingency(cross_tab)
    print(f'Chi-Square Test for {col} vs loan_status:')
    print(f'Chi2 value: {chi2}')
    print(f'p-value: {p}\n')

# Visualisasi distribusi nilai pada variabel target
fig, ax = plt.subplots(figsize=(12, 8))
sns.countplot(y='loan_status', data=df, order=df['loan_status'].value_counts().index, palette='plasma', ax=ax)

for p in ax.patches:
    ax.annotate(f'{int(p.get_width())}',
                (p.get_x() + p.get_width(), p.get_y() + 0.5),
                ha='left', va='center', color='black', fontsize=10)

ax.set_title('Distribusi Loan Status', fontsize=16, fontweight='bold')
ax.set_xlabel('Jumlah', fontsize=12)
ax.set_ylabel('Loan Status', fontsize=12)

plt.show()

"""**Conclusion from Descriptive Statistics**
1. The data consists of 466285 rows and 75 columns
2. Assumes 18 columns that are relevant for EDA (8 categorical columns, 10 numerical columns, and 2 datadate columns)
3. 10 numerical columns consisting of: loan_amnt, int_rate, annual_inc, dti, delinq_2yrs, open_acc, total_acc, revol_util, mths_since_last_delinq, collections_12_mths_ex_med
4. 8 categorical columns, consisting of: term, grade, home_ownership, emp_length, pymnt_plan, purpose, title, verification_status

## Univariate Analysis & Multivariate Analysis
"""

# analisis setiap kolom numerik untuk melihat distribusi nilai
features_nums = numerical_cols

n = len(features_nums)
ncols = 4
nrows = n // ncols if n % ncols == 0 else n // ncols + 1

fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, nrows*5))

for i, feature in enumerate(features_nums):
    r = i // ncols
    c = i % ncols
    ax = axs[r, c]
    sns.boxplot(y=df[feature], ax=ax)
    ax.set_title(feature)
    ax.set_ylabel('Nilai')

if n % ncols != 0:
    for j in range(n, nrows*ncols):
        fig.delaxes(axs.flatten()[j])

plt.suptitle('Distribusi Nilai Kolom Numerik', y=1.02, fontsize=16)
plt.tight_layout()
plt.show()

# Visualaliasi distribusi fitur-fitur numerik dalam features_nums
fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, nrows*5))

for i, feature in enumerate(features_nums):
    r = i // ncols
    c = i % ncols
    ax = axs[r, c]
    sns.kdeplot(x=df[feature], ax=ax)
    ax.set_xlabel(feature)
    ax.set_ylabel('Density')
    ax.set_title(f'Distribusi {feature}')

if n % ncols != 0:
    for j in range(n, nrows*ncols):
        fig.delaxes(axs.flatten()[j])

plt.tight_layout()
plt.show()

"""# Data Preparation üîÑ

* Handle missing values, either by removing them or fill in with appropriate imputation techniques.
* Address outliers if necessary, either by delete it or adjust its value.
* Encoding the categorical variable if required.
* Perform scaling or normalization on numerical features if needed.
* Divide data into train set and test set.

## Handling Missing Value
"""

cols_misval = ['desc','mths_since_last_record','mths_since_last_major_derog','annual_inc_joint','dti_joint','verification_status_joint','open_acc_6m','open_il_6m','open_il_12m','open_il_24m','mths_since_rcnt_il','total_bal_il','il_util','open_rv_12m','open_rv_24m','max_bal_bc','all_util','inq_fi','total_cu_tl','inq_last_12m']
df.drop(columns=cols_misval,inplace=True)
df_prep = df.copy()
df_prep.info()

"""**Deletes columns with more than 70% null data**

- *Data Quality :* Columns with a large amount of missing data may not provide enough information for machine learning.
- *Bias :* If we try to fill in missing data with multiple imputation methods, there is a big risk that we might introduce bias into our data.
- *Model Complexity :* Removing columns with a lot of missing data can help reduce the dimensionality of the data and can help prevent overfitting.

A threshold of 70% might be chosen as a balance point between retaining as much information as possible and avoiding noise or bias that might be introduced by missing data.

**Why we do this**‚ùì

We make a copy of the train data, namely `df_prep` to keep the original train data, the data we use for the next process is `df_prep`

### Handling Categorical Columns
"""

# Categorical columns
for col in df_prep.select_dtypes(include='object'):
    df_prep[col] = df_prep[col].fillna(df_prep[col].mode().iloc[0])
print("Updated Missing Values")
df_prep.isnull().sum()

"""**Why we do this**‚ùì

In categorical data, we fill in the missing values with the mode value

### Handling Numeric Columns
"""

# Numerical columns
for col in df_prep.select_dtypes(exclude='object'):
    df_prep[col] = df_prep[col].fillna(df_prep[col].median())
df_prep.isnull().sum()

"""**Why we do this**‚ùì

In numerical data, we fill in the missing values with the median value

### Handling Target Variable Column
"""

# loan_menjadi target karena terdapat informasi pembayaran konsumen
df_prep["loan_status"].unique()

# encoding target variable
target_dict = {'Fully Paid':1,
               'Does not meet the credit policy. Status:Fully Paid':1,
               'Charged Off':0,
               'Does not meet the credit policy. Status:Charged Off':0,
               'Default':0,
               'Late (31-120 days)':0,
               'Current':1,
               'In Grace Period':1,
               'Late (16-30 days)':0}
# Buat nilai yang dipetakan di kolom baru
df_prep['loan_status'] = df_prep['loan_status'].map(target_dict)
# Review dataset
df_prep.head()

"""Classifying each loan in the dataset into two main categories, namely "Good Loans (1)" and "Bad Loans (0)", based on the final status of each loan. This classification is the first step in understanding and managing credit risk. Classification Criteria:

Good Loans (1):
- 'Current': Loans that are still ongoing and payments are made according to schedule.
- 'Fully Paid': A loan that has been fully repaid by the borrower.
- 'In Grace Period': Loans that are still within the repayment grace period.
- 'Does not meet the credit policy. Status:Fully Paid': A loan that, although it does not meet the credit policy, has been paid in full.

Bad Loans (0):
- 'Charged Off': A loan that is deemed irrecoverable and written off as a loss.
- 'Late (16-30 days)': Loans that have a payment delay of between 16 to 30 days.
- 'Late (31-120 days)': Loans that have a late payment between 31 and 120 days.
- 'Default': A loan that cannot be recovered and is considered to be in default.
- 'Does not meet the credit policy. Status:Charged Off': Loans that, despite not meeting credit policy, are considered non-recoverable.

## Feature Engineering
"""

# Bulan dimana batas kredit peminjam yang paling awal dilaporkan dibuka
df_prep['earliest_cr_line'].value_counts()
df_prep['earliest_cr_line'] = pd.to_datetime(df_prep['earliest_cr_line'], format='%b-%y')

def date_time(dt):
  if dt.year > 2016:
    dt = dt.replace(year=dt.year-100)
  return dt

# Set standard datetime
df_prep['earliest_cr_line'] = pd.to_datetime(df_prep['earliest_cr_line'], format='%b-%y') # Bulan dimana batas kredit peminjam yang paling awal dilaporkan dibuka
df_prep['earliest_cr_line'] = df_prep['earliest_cr_line'].apply(lambda x: date_time(x))
df_prep['issue_d'] = pd.to_datetime(df_prep['issue_d'], format='%b-%y') # Bulan Pendanaan
df_prep['last_pymnt_d'] = pd.to_datetime(df_prep['last_pymnt_d'],format='%b-%y') # Pembayaran Bulan lalu yang telah diterima
df_prep['last_credit_pull_d'] = pd.to_datetime(df_prep['last_credit_pull_d'],format='%b-%y') # Bulan terakhir LC menarik kredit untuk pinjaman ini
df_prep[['earliest_cr_line','issue_d','last_pymnt_d','last_credit_pull_d']].head(5)

"""**A new column is needed for datetime:**

* **`pymnt_time`** = number of months between the loan being funded (**`issue_d`**) and the last payment received (**`last_pymnt_d`**)
* **`credit_pull_year`** = number of years between the borrower's earliest reported open credit line (**`earliest_cr_line`**) and the most recent LC that pulled credit for this loan (**`last_credit_pull_d`**)
"""

def diff_month(d1, d2):
    return (d1.year - d2.year) * 12 + d1.month - d2.month

def diff_year(d1, d2):
    return (d1.year - d2.year)

((df_prep.apply(lambda x: diff_month(x.last_pymnt_d, x.issue_d), axis=1) < 0)).any().any()
((df_prep.apply(lambda x: diff_month(x.last_credit_pull_d, x.earliest_cr_line), axis=1) < 0)).any().any()

df_prep['pymnt_time'] = df_prep.apply(lambda x: diff_month(x.last_pymnt_d, x.issue_d), axis=1)
df_prep['credit_pull_year'] = df_prep.apply(lambda x: diff_year(x.last_credit_pull_d, x.earliest_cr_line), axis=1)
print('Adding features succeed')

df_prep.info()

df_prep.to_csv('df_clean.csv', index=False)
!cp 'df_clean.csv' '/content/drive/MyDrive/Rakamin Academy/Data Scientist - ID X Partners/dataset/'
print('Saving cleaned data is done!')

df_clean = pd.read_csv("/content/drive/MyDrive/Rakamin Academy/Data Scientist - ID X Partners/dataset/df_clean.csv")
df_clean.head()

df_clean.describe(exclude=['int','float'])

df_clean.dtypes.value_counts()

non_used = ['Unnamed: 0','id','member_id','policy_code', 'loan_status']
uni_dist = df_clean.select_dtypes(include=[np.float64,np.int64])
uni_dist = uni_dist[uni_dist.columns[~uni_dist.columns.isin(non_used)]]

"""## Correlation Matrix"""

non_used = ['Unnamed: 0','id','member_id','policy_code','loan_status']
uni_dist = df_clean.select_dtypes(include=[np.float64,np.int64])
uni_dist = uni_dist[uni_dist.columns[~uni_dist.columns.isin(non_used)]]
fig = plt.figure(figsize = (40,10))
sns.heatmap(uni_dist.corr(),cmap='coolwarm', annot = True);

def top_correlation (df_clean,n):
    corr_matrix = df_clean.corr()
    correlation = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
                 .stack()
                 .sort_values(ascending=False))
    correlation = pd.DataFrame(correlation).reset_index()
    correlation.columns=["Variable_1","Variable_2","Correlation"]
    correlation = correlation.reindex(correlation.Correlation.abs().sort_values(ascending=False).index).reset_index().drop(["index"],axis=1)
    return correlation.head(n)
print("High Correlated Features (Corr > 0.5)")
top_correlation(uni_dist,39)

df_clean_corr = top_correlation(uni_dist,41)
df_clean_corr.to_excel('df_clean_corr.xlsx', index=False)
!cp 'df_clean_corr.xlsx' '/content/drive/MyDrive/Rakamin Academy/Data Scientist - ID X Partners/dataset/'
print('Saving correlation data is done!')

# Menghapus feature yang memiliki lebih dari 500 unique value dan 1 unique value

removed_unused = ['Unnamed: 0','id','member_id','next_pymnt_d','policy_code','emp_title','url','title','zip_code','earliest_cr_line']
multicol = ['last_credit_pull_d','last_pymnt_d','issue_d','addr_state','application_type',
            'out_prncp_inv','funded_amnt','total_pymnt_inv','funded_amnt_inv','total_rec_prncp','out_prncp',
            'revol_bal','total_pymnt','recoveries','total_rec_int','total_acc','loan_amnt']
removed_all = removed_unused + multicol

df_cleaned = df_clean[df_clean.columns[~df_clean.columns.isin(removed_all)]].reset_index(drop=True)

df_cleaned.to_csv('df_cleaned.csv', index=False)
!cp 'df_cleaned.csv' '/content/drive/MyDrive/Rakamin Academy/Data Scientist - ID X Partners/dataset/'
print('Saving cleaned data is done!')

"""## Encoding

One-hot encoding is used to convert categorical variables into a form that can be processed by a machine learning model, by substituting the categorical variables into a binary column (0 or 1) for each possible category. This is necessary because most machine learning algorithms work with numerical data and cannot handle categorical variables directly. With one-hot encoding, category information can be represented numerically without causing interpretation problems.
"""

df_cleaned = pd.read_csv('/content/drive/MyDrive/Rakamin Academy/Data Scientist - ID X Partners/dataset/df_cleaned.csv')
df_cleaned.head()

df_cleaned["term"].unique()

def text_num(text):
  return [int(s) for s in text.split() if s.isdigit()][0]

df_cleaned["term"] = df_cleaned["term"].apply(lambda x: text_num(x))
df_cleaned.head(5)

df_cleaned["grade"].unique()

# Define a dictionary for encoding ordinal variable
target_dict = {'A':6,
               'B':5,
               'C':4,
               'D':3,
               'E':2,
               'F':1,
               'G':0}
# Create the mapped values in a new column
df_cleaned["grade"] = df_cleaned["grade"].map(target_dict)

df_cleaned["sub_grade"].unique()

def f_A(row):
    if row == 'A1':
        val = 1
    elif row == 'A2':
        val = 2
    elif row == 'A3':
        val = 3
    elif row == 'A4':
        val = 4
    elif row == 'A5':
        val = 5
    else:
        val = 0
    return val

def f_B(row):
    if row == 'B1':
        val = 1
    elif row == 'B2':
        val = 2
    elif row == 'B3':
        val = 3
    elif row == 'B4':
        val = 4
    elif row == 'B5':
        val = 5
    else:
        val = 0
    return val

def f_C(row):
    if row == 'C1':
        val = 1
    elif row == 'C2':
        val = 2
    elif row == 'C3':
        val = 3
    elif row == 'C4':
        val = 4
    elif row == 'C5':
        val = 5
    else:
        val = 0
    return val

def f_D(row):
    if row == 'D1':
        val = 1
    elif row == 'D2':
        val = 2
    elif row == 'D3':
        val = 3
    elif row == 'D4':
        val = 4
    elif row == 'D5':
        val = 5
    else:
        val = 0
    return val

def f_E(row):
    if row == 'E1':
        val = 1
    elif row == 'E2':
        val = 2
    elif row == 'E3':
        val = 3
    elif row == 'E4':
        val = 4
    elif row == 'E5':
        val = 5
    else:
        val = 0
    return val

def f_F(row):
    if row == 'F1':
        val = 1
    elif row == 'F2':
        val = 2
    elif row == 'F3':
        val = 3
    elif row == 'F4':
        val = 4
    elif row == 'F5':
        val = 5
    else:
        val = 0
    return val

def f_G(row):
    if row == 'G1':
        val = 1
    elif row == 'G2':
        val = 2
    elif row == 'G3':
        val = 3
    elif row == 'G4':
        val = 4
    elif row == 'G5':
        val = 5
    else:
        val = 0
    return val

df_cleaned['SubGrade_A'] = df_cleaned["sub_grade"].apply(f_A)
df_cleaned['SubGrade_B'] = df_cleaned["sub_grade"].apply(f_B)
df_cleaned['SubGrade_C'] = df_cleaned["sub_grade"].apply(f_C)
df_cleaned['SubGrade_D'] = df_cleaned["sub_grade"].apply(f_D)
df_cleaned['SubGrade_E'] = df_cleaned["sub_grade"].apply(f_E)
df_cleaned['SubGrade_F'] = df_cleaned["sub_grade"].apply(f_F)
df_cleaned['SubGrade_G'] = df_cleaned["sub_grade"].apply(f_G)
df_cleaned = df_cleaned.drop(axis=1, columns="sub_grade")

df_cleaned["emp_length"].unique()

# Define a dictionary for encoding ordinal variable
target_dict = {'< 1 year':0,
               '1 year':1,
               '2 years':2,
               '3 years':3,
               '4 years':4,
               '5 years':5,
               '6 years':6,
               '7 years':7,
               '8 years':8,
               '9 years':9,
               '10+ years':10}
# Create the mapped values in a new column
df_cleaned["emp_length"] = df_cleaned["emp_length"].map(target_dict)

df_cleaned["home_ownership"].unique()

# Define a dictionary for aggregating variable
target_dict = {'MORTGAGE':'MORTGAGE',
               'RENT':'RENT',
               'OWN':'OWN',
               'OTHER':'OTHER',
               'ANY':'OTHER',
               'NONE':'OTHER'}
# Create the mapped values in a new column
df_cleaned["home_ownership"] = df_cleaned["home_ownership"].map(target_dict)

df_cleaned = pd.concat([df_cleaned, pd.get_dummies(df_cleaned['home_ownership'], prefix='home_ownership')], axis=1)
df_cleaned.drop(['home_ownership'], axis=1, inplace=True)
df_cleaned.head()

df_cleaned["verification_status"].unique()

df_cleaned = pd.concat([df_cleaned, pd.get_dummies(df_cleaned["verification_status"], prefix="verification_status")], axis=1)
df_cleaned.drop(["verification_status"], axis=1, inplace=True)
df_cleaned.head()

df_cleaned['pymnt_plan'].unique()

# Define a dictionary for encoding ordinal variable
target_dict = {'n':0,
               'y':1}
# Create the mapped values in a new column
df_cleaned["pymnt_plan"] = df_cleaned["pymnt_plan"].map(target_dict)

df_cleaned["loan_status"].unique()

df_cleaned["purpose"].unique()

# Define a dictionary for aggregating variable
target_dict = {'debt_consolidation':'debt_consolidation',
               'credit_card':'credit_card',
               'home_improvement':'private_use',
               'other':'other',
               'major_purchase':'major_purchase',
               'small_business':'small_business',
               'car':'private_use',
               'medical':'private_use',
               'wedding':'private_use',
               'moving':'private_use',
               'house':'private_use',
               'vacation':'private_use',
               'educational':'private_use',
               'renewable_energy':'other'}
# Create the mapped values in a new column
df_cleaned["purpose"] = df_cleaned["purpose"].map(target_dict)

df_cleaned = pd.concat([df_cleaned, pd.get_dummies(df_cleaned['purpose'], prefix='purpose')], axis=1)
df_cleaned.drop(['purpose'], axis=1, inplace=True)
df_cleaned.head()

df_cleaned["initial_list_status"].unique()

df_cleaned = pd.concat([df_cleaned, pd.get_dummies(df_cleaned["initial_list_status"], prefix="initial_list_status")], axis=1)
df_cleaned.drop(["initial_list_status"], axis=1, inplace=True)
df_cleaned.head()

df_cleaned.info()

"""## Feature Importance

IV and WoE are metrics commonly used in credit analysis to assess the predictive power of an independent variable against a target variable.

- *Information Value (IV):* Measures how well a variable can differentiate between two classes in the target variable. The higher the IV value, the better the variable is at predicting the target variable.

- *Weight of Evidence (WoE):* Measures the strength of the relationship between the independent variable and the target variable. WoE describes the logarithmic change in odds between two classes of target variables.
"""

# Implementasikan perhitungan Information Value (IV) dan Weight of Evidence (WoE) untuk setiap variabel independen dalam dataset.
def iv_woe(data, target, bins=10, show_woe=False):

    newDF, woeDF = pd.DataFrame(), pd.DataFrame()
    cols = data.columns

    # Run WoE and IV
    for ivars in cols[~cols.isin([target])]:
        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):
            binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')
            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})
        else:
            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})
        d = d0.groupby("x", as_index=False).agg({"y": ["count", "sum"]})
        d.columns = ['Cutoff', 'N', 'Events']
        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()
        d['Non-Events'] = d['N'] - d['Events']
        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()
        d['WoE'] = np.log(d['% of Events']/d['% of Non-Events'])
        d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])
        d.insert(loc=0, column='Variable', value=ivars)
        print("Information value of " + ivars + " is : " + str(round(d['IV'].sum(),6)))
        temp = pd.DataFrame({"Variable" : [ivars], "IV" : [d['IV'].sum()]}, columns = ["Variable", "IV"])
        newDF = pd.concat([newDF, temp], axis=0)
        woeDF = pd.concat([woeDF, d], axis=0)

        if show_woe == True:
            print(d)

    return newDF, woeDF

iv, woe = iv_woe(df_cleaned, target='loan_status', bins=20)

"""The general rule states that all variables with IV < 0.02 are useless for prediction and IV > 0.5 have suspicious predictive power. Therefore, the following variables will not be included: pymnt_plan, last_pymnt_amnt, revol_util, delinq_2yrs, mths_since_last_delinq, open_acc, pub_rec, collections_12_mths_ex_med, acc_now_delinq, tot_coll_amt, mths_since_last_pymnt_d, emp_length, application_type."""

df_cleaned.drop(columns=['installment', 'emp_length', 'pymnt_plan', 'last_pymnt_amnt', 'delinq_2yrs', 'mths_since_last_delinq',
                    'open_acc', 'pub_rec', 'total_rec_late_fee', 'collection_recovery_fee','collections_12_mths_ex_med', 'acc_now_delinq',
                    'tot_coll_amt', 'pymnt_time'], axis=1, inplace=True)

"""## Feature Scaling

1. Normalization is carried out to overcome differences in data scale, because normalization will bring the data to a uniform range between 0 and 1. This will help machine learning algorithms that are sensitive to scale differences in data, such as logistic regression and k-nearest neighbors, to produce better results. Apart from that, normalization is also suitable for data with a non-normal or bimodal distribution.
2. Standardization because it has a distribution close to normal. Standardization changes the data into a standard normal distribution with a mean of 0 and a standard deviation of 1, thereby meeting the assumptions of machine learning algorithms which assume a normal distribution. With standardization, these variables will have a balanced impact on modeling.
"""

min_max_scaler = MinMaxScaler()
standard_scaler = StandardScaler()

# Min-Max
df_cleaned['term'] = min_max_scaler.fit_transform(df_cleaned[['term']])
df_cleaned['annual_inc'] = min_max_scaler.fit_transform(df_cleaned[['annual_inc']])
df_cleaned['inq_last_6mths'] = min_max_scaler.fit_transform(df_cleaned[['inq_last_6mths']])
df_cleaned['tot_cur_bal'] = min_max_scaler.fit_transform(df_cleaned[['tot_cur_bal']])
df_cleaned['total_rev_hi_lim'] = min_max_scaler.fit_transform(df_cleaned[['total_rev_hi_lim']])
df_cleaned['credit_pull_year'] = min_max_scaler.fit_transform(df_cleaned[['credit_pull_year']])

# Standarization
df_cleaned['int_rate'] = standard_scaler.fit_transform(df_cleaned[['int_rate']])
df_cleaned['dti'] = standard_scaler.fit_transform(df_cleaned[['dti']])

"""1. Normalize columns that have a skew distribution > 0.5 or > -0.5
2. Standardize columns that have a normal distribution between 0.5 to -0.5
"""

df_fix = df_cleaned.copy()

df_cleaned.to_csv('df_fix.csv', index=False)
!cp 'df_fix.csv' '/content/drive/MyDrive/Rakamin Academy/Data Scientist - ID X Partners/dataset/'
print('Saving data is done!')

"""## Train-Test Split

70% Training + 30% Testing
"""

df_fix

# Pisahkan fitur dan variabel target (df_train)
df_train_feat = df_fix.loc[:, df_fix.columns != "loan_status"]
df_train_target = df_fix["loan_status"]

df_train_feat.to_csv('df_train_feat.csv', index=False)
!cp 'df_train_feat.csv' '/content/drive/MyDrive/Rakamin Academy/Data Scientist - ID X Partners/dataset/'

df_train_target.to_csv('df_train_target.csv', index=False)
!cp 'df_train_target.csv' '/content/drive/MyDrive/Rakamin Academy/Data Scientist - ID X Partners/dataset/'
print('Saving data is done!')

from collections import Counter
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_train_feat, df_train_target, test_size=0.2,
                                                    random_state=42, stratify=df_train_target)
print('Class from training data df_train',Counter(y_train))

print('Class from testing data df_test',Counter(y_test))

"""## Handling Imbalance

SMOTE, or Synthetic Minority Over-sampling Technique, is used to deal with class imbalance in a dataset. Class imbalance can cause the model to have a tendency to discriminate or ignore minority classes, thereby harming the model's performance in predicting those classes.

From the previous visualization, it can be seen that there is an imbalance in the classes, therefore over sampling using SMOTE is used to equalize the class distribution and hopefully improve model performance in the minority class.
"""

# Distribusi of training target
plt.figure(figsize=(6,6))
plt.pie(
        y_train.value_counts(),
        autopct='%.2f',
        explode=[0.1,0],
        labels=["Yes","No"],
        shadow=True,
        textprops={'fontsize': 14},
        colors=["orange","red"],
        startangle=35)

plt.title("Proporsi Target Kelas",fontsize=20, fontweight='bold', pad=20)
plt.show()

# Implementasi Smote
smote = SMOTE(random_state=42)
x_smote, y_smote = smote.fit_resample(X_train, y_train)

class_distribution_before = y_train.value_counts().reset_index()
class_distribution_before.columns = ['Loan Status', 'Count']

# Data setelah SMOTE
class_distribution_after = pd.Series(y_smote).value_counts().reset_index()
class_distribution_after.columns = ['Loan Status', 'Count']

# Plotting distribusi kelas sebelum dan sesudah SMOTE menggunakan plotly.express
fig = px.bar(class_distribution_before, x='Loan Status', y='Count', title='Sebelum SMOTE',
            labels={'Loan Status': 'Loan Status', 'Count': 'Jumlah Sampel'},
            color='Loan Status', color_discrete_sequence=['red'])
fig.update_layout(showlegend=False)

fig2 = px.bar(class_distribution_after, x='Loan Status', y='Count', title='Setelah SMOTE',
            labels={'Loan Status': 'Loan Status', 'Count': 'Jumlah Sampel'},
            color='Loan Status', color_discrete_sequence=['blue'])
fig2.update_layout(showlegend=False)

fig.show()
fig2.show()

"""# Data Modelling üéØ

* Choose a machine learning model that suits your type problems and goals to be achieved.
* Perform model training on the training set.
* Adjust model parameters using techniques such as cross-validation or grid search.
* Evaluate model performance using metrics relevant such as accuracy, precision, recall, or ROC-AUC.
* Checking for overfitting or underfitting of the model and take appropriate action.

## Baseline Model Build
"""

training_score = []
testing_score = []
precission = []
recall = []
Roc_Auc_score = []
f1_score_ = []

"""## Model Building - Unscaled Data"""

from sklearn.model_selection import StratifiedKFold, cross_val_predict
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix, classification_report, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

def model_prediction(model, x, y, n_splits=5, random_state=42):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    for train_index, test_index in skf.split(x, y):
        x_train, x_test = x.iloc[train_index], x.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        model.fit(x_train, y_train)
        x_train_pred = model.predict(x_train)
        x_test_pred = model.predict(x_test)
        y_test_prob = model.predict_proba(x_test)[:, 1]

        a = accuracy_score(y_train, x_train_pred) * 100
        b = accuracy_score(y_test, x_test_pred) * 100
        c = precision_score(y_test, x_test_pred)
        d = recall_score(y_test, x_test_pred)
        e = roc_auc_score(y_test, y_test_prob)
        f = f1_score(y_test, x_test_pred)

        training_score.append(a)
        testing_score.append(b)
        precission.append(c)
        recall.append(d)
        Roc_Auc_score.append(e)
        f1_score_.append(f)

    print("\n------------------------------------------------------------------------")
    print(f"Mean Accuracy_Score of {model} model on Training Data is:", np.mean(training_score))
    print(f"Mean Accuracy_Score of {model} model on Testing Data is:", np.mean(testing_score))
    print(f"Mean Precision Score of {model} model is:", np.mean(precission))
    print(f"Mean Recall Score of {model} model is:", np.mean(recall))
    print(f"Mean ROC_AUC Score of {model} model is:", np.mean(Roc_Auc_score))
    print(f"Mean f1 Score of {model} model is:", np.mean(f1_score_))

    print("\n------------------------------------------------------------------------")
    print(f"Classification Report of {model} model is:")
    y_pred_all = cross_val_predict(model, x, y, cv=skf)
    print(classification_report(y, y_pred_all))

    print("\n------------------------------------------------------------------------")
    print(f"Confusion Matrix of {model} model is:")
    cm = confusion_matrix(y, y_pred_all)
    plt.figure(figsize=(8, 4))
    sns.heatmap(cm, annot=True, fmt="g", cmap="summer")
    plt.show()

"""### Logistic Regression"""

model_prediction(LogisticRegression(), x_smote, y_smote, n_splits=5, random_state=42)

"""### Light Gradient Boosting Machine"""

model_prediction(LGBMClassifier(), x_smote, y_smote, n_splits=5, random_state=42)

"""### Random Forest"""

model_prediction(RandomForestClassifier(), x_smote, y_smote, n_splits=5, random_state=42)

"""### Extra Trees"""

model_prediction(ExtraTreesClassifier(), x_smote, y_smote, n_splits=5, random_state=42)

"""## Loan Status Distribution"""

# Instantiate the ExtraTreesClassifier
et_model = ExtraTreesClassifier(random_state=42)
et_model.fit(x_smote, y_smote)
et_pred = et_model.predict(df_train_feat)

df_final = pd.DataFrame()
df_final['loan_status'] = et_pred

# Menghitung frekuensi setiap kategori
value_counts = df_final['loan_status'].value_counts()

# Membuat pie chart
plt.figure(figsize=(8, 8))
plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%')
plt.title('Distribusi Status Pinjaman')
plt.show()

"""# Evaluation ‚úîÔ∏è

* Evaluate model performance against dataset separate testing.
* Analyze the evaluation metrics obtained for evaluate how well the model works.
* Prepare a summary of the evaluation results and present it clearly.

Based on the modeling results obtained, it is known that:

- **Evaluate Model Performance Against Split Test Dataset**:
   - The model shows **95.98%** average accuracy on test data, indicating excellent performance.
   - The confusion matrix shows a high number of **True Positive** and **True Negative**, indicating accurate predictions.

- **Evaluation Metrics Analysis**:
   - High **Precision** (0.958764273972426) indicates that the model has a low **False Positive** rate.
   - High **recall** indicates that the model was successful in identifying the majority of true positive cases.
   - A high **F1-score** indicates a good balance between precision and recall.

- **Summary of Evaluation Results**:
   - The **ExtraTreesClassifier** model has excellent performance with high accuracy on both training and testing datasets.
   - High precision, recall and f1-score scores indicate that the model has strong predictive power.
   - The confusion matrix confirms good performance by showing high numbers on the main diagonal compared to off-diagonal elements.

This summary shows that the classification model used works well and may not require further tuning, unless there is a specific goal to be achieved or to improve performance in certain cases.
"""